import asyncio
import json
import requests
import sounddevice as sd
import numpy as np
import websockets
from hum import record_hum
from midi import convert_to_midi
import time
import warnings
import traceback
import threading

warnings.filterwarnings("ignore", category=UserWarning, module="sounddevice")

API_KEY = "8461cb9f-7560-4c62-a120-7173b2696850"
ASSISTANT_ID = "a6210139-4abf-4ed8-b1a5-0996953045b3"
CALL_URL = "https://api.vapi.ai/call"


def create_ws_call():
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
    payload = {
        "assistantId": ASSISTANT_ID,
        "transport": {
            "provider": "vapi.websocket",
            "audioFormat": {
                "format": "pcm_s16le",
                "container": "raw",
                "sampleRate": 16000,
            },
        },
    }
    resp = requests.post(CALL_URL, headers=headers, json=payload)
    resp.raise_for_status()
    data = resp.json()
    print("‚úÖ Call started:", data)
    ws_url = data["transport"]["websocketCallUrl"]
    print("üîó Connect to:", ws_url)
    return ws_url


async def audio_producer(queue: asyncio.Queue):
    """Capture mic and push PCM chunks into queue"""

    def callback(indata, frames, time, status):
        if status:
            print("‚ö†Ô∏è Audio status:", status)
        audio_int16 = (indata[:, 0] * 32767).astype(np.int16)
        queue.put_nowait(audio_int16.tobytes())

    with sd.InputStream(samplerate=16000, channels=1, dtype="float32", callback=callback):
        print("üé§ Recording... press Ctrl+C to stop")
        await asyncio.Future()  # run forever


async def audio_consumer(ws, queue: asyncio.Queue):
    """Send mic audio from queue to websocket"""
    while True:
        chunk = await queue.get()
        await ws.send(chunk)


async def listen(ws):
    buffer = ""
    phrase_detected = False
    instrument_name = None

    with sd.OutputStream(samplerate=16000, channels=1, dtype="int16") as speaker:
        async for message in ws:
            if isinstance(message, (bytes, bytearray)):
                audio = np.frombuffer(message, dtype=np.int16)
                speaker.write(audio)
                continue

            try:
                data = json.loads(message)
            except Exception:
                continue

            if data.get("type") == "model-output":
                buffer += data.get("output", "")
                print("üß© Buffer:", buffer)

                if not phrase_detected and "detected, please hum in 5 seconds" in buffer:
                    instrument_name = buffer.split(" detected")[0].strip().split()[-1]
                    print(f"‚úÖ Instrument: {instrument_name}")
                    with open("instrument.txt", "w") as f:
                        f.write(instrument_name)
                    phrase_detected = True

            elif data.get("type") == "speech-update":
                if (
                    phrase_detected
                    and data.get("status") == "stopped"
                    and data.get("role") == "assistant"
                ):
                    print("üîö Assistant finished phrase, waiting 3s then closing...")
                    
                    await ws.close(code=1000, reason="done")
                    return  # exit listen()


async def main():
    ws_url = create_ws_call()
    queue = asyncio.Queue()

    try:
        async with websockets.connect(ws_url) as ws:
            await asyncio.gather(
                audio_producer(queue),
                audio_consumer(ws, queue),
                listen(ws),
            )
    except Exception as e:
        print("‚ö†Ô∏è Main loop ended:", e)


async def full_pipeline():
    """Complete pipeline: voice assistant -> hum recording -> MIDI -> CUA"""
    # Run websocket loop
    await main()

    # After WS closes, start hum recording
    
    # Get the instrument from the file (set by Vapi assistant)
    instrument = "piano"  # default fallback
    try:
        with open("instrument.txt", "r") as f:
            instrument = f.read().strip()
            print(f"üéµ Detected instrument: {instrument}")
    except FileNotFoundError:
        print("‚ö†Ô∏è No instrument.txt found, using default: piano")
    
    # Create timestamped WAV filename
    timestamp = int(time.time())
    wav_file = f"hum_{timestamp}.wav"
    
    # Record hum and process based on instrument (hum.py handles the routing)
    midi_file = record_hum(wav_file, instrument=instrument)
    
    if midi_file:
        print(f"‚úÖ MIDI file created: {midi_file}")
        
        # Send to CUA for DAW processing
        print("üöÄ Sending to CUA for DAW processing...")
        # from cua import run_cua
        # await run_cua(midi_file, instrument)
    else:
        print("‚ùå Failed to create MIDI file")


def _run_full_pipeline_safe():
    try:
        asyncio.run(full_pipeline())
    except ModuleNotFoundError as e:
        print("‚ùå Missing module (needed by CUA or other backend):", e)
        traceback.print_exc()
    except Exception as e:
        print("‚ùå Unexpected error inside listen full_pipeline():", e)
        traceback.print_exc()

def start_listening():
    """
    Entrypoint to start the backend pipeline. Designed to be called from a
    separate process (recommended) to isolate TensorFlow / PortAudio.
    """
    print("üì° listen.py start_listening() called")
    _run_full_pipeline_safe()

def start_listening(callback_on_done=None):
    """
    Start the full pipeline in a background thread.
    callback_on_done: function to call after MIDI is created (e.g., to re-enable the button)
    """

    def runner():
        try:
            asyncio.run(full_pipeline())
        except Exception as e:
            print("‚ùå Error in full_pipeline():", e)
        finally:
            if callback_on_done:
                callback_on_done()  # notify the GUI

    thread = threading.Thread(target=runner, daemon=True)
    thread.start()

def start_listening():
    asyncio.run(full_pipeline())
